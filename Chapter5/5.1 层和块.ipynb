{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "下面的代码生成一个网络，其中包含一个具有256个单元和ReLU激活函数的全连接隐藏层， 然后是一个具有10个隐藏单元且不带激活函数的全连接输出层。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0bae36c92fe50b5"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.0895, -0.1235,  0.0124,  0.0969, -0.2384, -0.0993, -0.0759,  0.1079,\n         -0.1750, -0.0694],\n        [-0.0634, -0.1938,  0.0287,  0.0679, -0.1898, -0.2544, -0.0326,  0.2642,\n         -0.0689,  0.0274]], grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "\n",
    "X = torch.rand(2, 20)\n",
    "net(X)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T11:23:06.586293600Z",
     "start_time": "2023-10-07T11:23:04.275998700Z"
    }
   },
   "id": "9aae638334fafd7a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 自定义块"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3e192f211383ebc"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-07T11:23:06.630381700Z",
     "start_time": "2023-10-07T11:23:06.591281100Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    # 用模型参数声明层。这里，我们声明两个全连接的层\n",
    "    def __init__(self):\n",
    "        # 调用MLP的父类Module的构造函数来执行必要的初始化。\n",
    "        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)  # 隐藏层\n",
    "        self.out = nn.Linear(256, 10)  # 输出层\n",
    "\n",
    "    # 定义模型的前向传播，即如何根据输入X返回所需的模型输出\n",
    "    def forward(self, X):\n",
    "        # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.0450, -0.0375, -0.2047, -0.1443, -0.0648, -0.0176, -0.0774, -0.2276,\n         -0.0647, -0.1676],\n        [-0.1380,  0.1124, -0.2106, -0.0969, -0.1685, -0.0109, -0.2363, -0.2737,\n         -0.0174, -0.1878]], grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net(X)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T11:23:15.333036400Z",
     "start_time": "2023-10-07T11:23:15.283170300Z"
    }
   },
   "id": "58cc5c0c381f6d65"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 顺序块"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "822a04971b1e589e"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for idx, module in enumerate(args):\n",
    "            # 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员\n",
    "            # 变量_modules中。_module的类型是OrderedDict\n",
    "            self._modules[str(idx)] = module\n",
    "\n",
    "    def forward(self, X):\n",
    "        # OrderedDict保证了按照成员添加的顺序遍历它们\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T11:25:12.215745100Z",
     "start_time": "2023-10-07T11:25:12.184827100Z"
    }
   },
   "id": "830bbc54a9619a70"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.2427, -0.3236,  0.2510,  0.2417,  0.2294,  0.0615,  0.2295,  0.1248,\n          0.0686,  0.1844],\n        [ 0.0931, -0.0721,  0.3505,  0.2057,  0.1988, -0.1600,  0.1578,  0.1883,\n          0.0497,  0.1953]], grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "net(X)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T11:25:20.516971300Z",
     "start_time": "2023-10-07T11:25:20.467902300Z"
    }
   },
   "id": "bcc274136a1fd1e1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 在前向传播函数中执行代码"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d5ffa521ea9a199e"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 不计算梯度的随机权重参数。因此其在训练期间保持不变\n",
    "        self.rand_weight = torch.rand((20, 20), requires_grad=False)\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        # 使用创建的常量参数以及relu和mm函数\n",
    "        X = F.relu(torch.mm(X, self.rand_weight) + 1)\n",
    "        # 复用全连接层。这相当于两个全连接层共享参数\n",
    "        X = self.linear(X)\n",
    "        # 控制流\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T11:25:55.460194Z",
     "start_time": "2023-10-07T11:25:55.438490800Z"
    }
   },
   "id": "1adf9517bd94d920"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.0772, grad_fn=<SumBackward0>)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T11:25:56.489006100Z",
     "start_time": "2023-10-07T11:25:56.434666700Z"
    }
   },
   "id": "23e2f68e1f8b5014"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(-0.2031, grad_fn=<SumBackward0>)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),\n",
    "                                 nn.Linear(64, 32), nn.ReLU())\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())\n",
    "chimera(X)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-07T11:30:08.547679400Z",
     "start_time": "2023-10-07T11:30:08.498332800Z"
    }
   },
   "id": "473777e9e63ae8e3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 小结\n",
    "\n",
    "- 一个块可以由许多层组成；一个块可以由许多块组成。\n",
    "- 块可以包含代码。\n",
    "- 块负责大量的内部处理，包括参数初始化和反向传播。\n",
    "- 层和块的顺序连接由Sequential块处理。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef9876745d4a3d7d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 练习\n",
    "\n",
    "1. 如果将MySequential中存储块的方式更改为Python列表，会出现什么样的问题？\n",
    "2. 实现一个块，它以两个块为参数，例如net1和net2，并返回前向传播中两个网络的串联输出。这也被称为平行块。\n",
    "3. 假设我们想要连接同一网络的多个实例。实现一个函数，该函数生成同一个块的多个实例，并在此基础上构建更大的网络。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29c394139b7c6ec0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "531221b998d55fdb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
