{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "在过去的几年里，出于对深度学习强烈的兴趣， 许多公司、学者和业余爱好者开发了各种成熟的开源框架。 这些框架可以自动化基于梯度的学习算法中重复性的工作。 在[3.2](http://zh.d2l.ai/chapter_linear-networks/linear-regression-scratch.html#sec-linear-scratch)节中，我们只运用了： \n",
    "    1. 通过张量来进行数据存储和线性代数；\n",
    "    2. 通过自动微分来计算梯度。 实际上，由于数据迭代器、损失函数、优化器和神经网络层很常用， 现代深度学习库也为我们实现了这些组件。\n",
    "\n",
    "本节将介绍如何通过使用深度学习框架来简洁地实现[3.2节](http://zh.d2l.ai/chapter_linear-networks/linear-regression-scratch.html#sec-linear-scratch)中的线性回归模型。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74cc72d1d603b2a3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 生成数据集\n",
    "\n",
    "与[3.2节](http://zh.d2l.ai/chapter_linear-networks/linear-regression-scratch.html#sec-linear-scratch)中类似，我们首先生成数据集。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e7b249e896f28f7"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from d2l import torch as d2l\n",
    "\n",
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "features, labels = d2l.synthetic_data(true_w, true_b, 1000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T10:18:37.119145300Z",
     "start_time": "2023-09-20T10:18:33.527398500Z"
    }
   },
   "id": "339062e4e8e16db8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 读取数据集\n",
    "\n",
    "我们可以调用框架中现有的API来读取数据。 我们将features和labels作为API的参数传递，并通过数据迭代器指定batch_size。 此外，布尔值is_train表示是否希望数据迭代器对象在每个迭代周期内打乱数据。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6baec25cf8daa3c9"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def load_array(data_arrays, batch_size, is_train=True):\n",
    "    \"\"\"构造一个PyTorch数据迭代器\"\"\"\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "\n",
    "batch_size = 10\n",
    "data_iter = load_array((features, labels), batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T10:20:25.611669300Z",
     "start_time": "2023-09-20T10:20:25.591506600Z"
    }
   },
   "id": "f8d84e37ed443c41"
  },
  {
   "cell_type": "markdown",
   "source": [
    "使用data_iter的方式与我们在[3.2节](http://zh.d2l.ai/chapter_linear-networks/linear-regression-scratch.html#sec-linear-scratch)中使用data_iter函数的方式相同。为了验证是否正常工作，让我们读取并打印第一个小批量样本。 与[3.2节](http://zh.d2l.ai/chapter_linear-networks/linear-regression-scratch.html#sec-linear-scratch)不同，这里我们使用iter构造Python迭代器，并使用next从迭代器中获取第一项。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79b8c3ad390699f1"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "[tensor([[-0.1678,  1.7272],\n         [-0.2178, -0.8052],\n         [-0.6036, -0.1643],\n         [ 1.2687,  0.2449],\n         [-0.5995, -0.1154],\n         [ 1.6624,  0.1052],\n         [ 1.1559, -0.7786],\n         [-0.5059,  0.4686],\n         [ 0.4880,  0.4750],\n         [ 0.6378,  0.0958]]),\n tensor([[-2.0032],\n         [ 6.4917],\n         [ 3.5588],\n         [ 5.9079],\n         [ 3.3900],\n         [ 7.1586],\n         [ 9.1581],\n         [ 1.5986],\n         [ 3.5484],\n         [ 5.1495]])]"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(data_iter))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T10:21:10.367797700Z",
     "start_time": "2023-09-20T10:21:10.309608700Z"
    }
   },
   "id": "b1668657a5d12350"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 定义模型\n",
    "\n",
    "当我们在[3.2节](http://zh.d2l.ai/chapter_linear-networks/linear-regression-scratch.html#sec-linear-scratch)中实现线性回归时， 我们明确定义了模型参数变量，并编写了计算的代码，这样通过基本的线性代数运算得到输出。 但是，如果模型变得更加复杂，且当我们几乎每天都需要实现模型时，自然会想简化这个过程。 这种情况类似于为自己的博客从零开始编写网页。 做一两次是有益的，但如果每个新博客就需要工程师花一个月的时间重新开始编写网页，那并不高效。\n",
    "\n",
    "对于标准深度学习模型，我们可以使用框架的预定义好的层。这使我们只需关注使用哪些层来构造模型，而不必关注层的实现细节。 我们首先定义一个模型变量net，它是一个Sequential类的实例。 Sequential类将多个层串联在一起。 当给定输入数据时，Sequential实例将数据传入到第一层， 然后将第一层的输出作为第二层的输入，以此类推。 在下面的例子中，我们的模型只包含一个层，因此实际上不需要Sequential。 但是由于以后几乎所有的模型都是多层的，在这里使用Sequential会让你熟悉“标准的流水线”。\n",
    "\n",
    "回顾[图3.1.2](http://zh.d2l.ai/chapter_linear-networks/linear-regression.html#fig-single-neuron)中的单层网络架构， 这一单层被称为全连接层（fully-connected layer）， 因为它的每一个输入都通过矩阵-向量乘法得到它的每个输出。\n",
    "\n",
    "在PyTorch中，全连接层在Linear类中定义。 值得注意的是，我们将两个参数传递到nn.Linear中。 第一个指定输入特征形状，即2，第二个指定输出特征形状，输出特征形状为单个标量，因此为1。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a8ac10d5f242592"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# nn是神经网络的缩写\n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(nn.Linear(2, 1))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T10:22:52.256923Z",
     "start_time": "2023-09-20T10:22:52.237972900Z"
    }
   },
   "id": "10b4ad879e987a4e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 初始化模型参数\n",
    "\n",
    "在使用net之前，我们需要初始化模型参数。 如在线性回归模型中的权重和偏置。 深度学习框架通常有预定义的方法来初始化参数。 在这里，我们指定每个权重参数应该从均值为0、标准差为0.01的正态分布中随机采样， 偏置参数将初始化为零。\n",
    "\n",
    "正如我们在构造nn.Linear时指定输入和输出尺寸一样， 现在我们能直接访问参数以设定它们的初始值。 我们通过net[0]选择网络中的第一个图层， 然后使用weight.data和bias.data方法访问参数。 我们还可以使用替换方法normal_和fill_来重写参数值。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "710fdb109fd48f7"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data.normal_(0, 0.01)\n",
    "net[0].bias.data.fill_(0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T10:25:03.166694Z",
     "start_time": "2023-09-20T10:25:03.117809100Z"
    }
   },
   "id": "af6ded755c62d286"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 定义损失函数\n",
    "\n",
    "计算均方误差使用的是MSELoss类，也称为平方$L_2$范数。 默认情况下，它返回所有样本损失的平均值。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8801d9ead215346"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T10:27:40.516816Z",
     "start_time": "2023-09-20T10:27:40.498864Z"
    }
   },
   "id": "d8f40cb71af2a792"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 定义优化算法\n",
    "\n",
    "小批量随机梯度下降算法是一种优化神经网络的标准工具， PyTorch在optim模块中实现了该算法的许多变种。 当我们实例化一个SGD实例时，我们要指定优化的参数 （可通过net.parameters()从我们的模型中获得）以及优化算法所需的超参数字典。 小批量随机梯度下降只需要设置lr值，这里设置为0.03。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a54a18fc1b2497d1"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "trainer = torch.optim.SGD(net.parameters(), lr=0.03)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T10:28:53.324335700Z",
     "start_time": "2023-09-20T10:28:53.307347300Z"
    }
   },
   "id": "bdbae71e58cac973"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 训练\n",
    "\n",
    "通过深度学习框架的高级API来实现我们的模型只需要相对较少的代码。 我们不必单独分配参数、不必定义我们的损失函数，也不必手动实现小批量随机梯度下降。 当我们需要更复杂的模型时，高级API的优势将大大增加。 当我们有了所有的基本组件，训练过程代码与我们从零开始实现时所做的非常相似。\n",
    "\n",
    "回顾一下：在每个迭代周期里，我们将完整遍历一次数据集（train_data）， 不停地从中获取一个小批量的输入和相应的标签。 对于每一个小批量，我们会进行以下步骤:\n",
    "\n",
    "- 通过调用net(X)生成预测并计算损失l（前向传播）。\n",
    "- 通过进行反向传播来计算梯度。\n",
    "- 通过调用优化器来更新模型参数。\n",
    "\n",
    "为了更好的衡量训练效果，我们计算每个迭代周期后的损失，并打印它来监控训练过程。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bcd12bee840da2c2"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.000266\n",
      "epoch 2, loss 0.000104\n",
      "epoch 3, loss 0.000103\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter:\n",
    "        # 前向传播，求预测并求损失\n",
    "        l = loss(net(X), y)\n",
    "        # 梯度清零\n",
    "        trainer.zero_grad()\n",
    "        # 反向传播，求梯度\n",
    "        l.backward()\n",
    "        # 更新模型参数\n",
    "        trainer.step()\n",
    "    l = loss(net(features), labels)\n",
    "    print(f'epoch {epoch + 1}, loss {l:f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T10:30:48.511073200Z",
     "start_time": "2023-09-20T10:30:48.356412600Z"
    }
   },
   "id": "28727a1a24689449"
  },
  {
   "cell_type": "markdown",
   "source": [
    "下面我们比较生成数据集的真实参数和通过有限数据训练获得的模型参数。 要访问参数，我们首先从net访问所需的层，然后读取该层的权重和偏置。 正如在从零开始实现中一样，我们估计得到的参数与生成数据的真实参数非常接近。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "575ac4ed8a7db703"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w的估计误差： tensor([0.0006, 0.0005])\n",
      "b的估计误差： tensor([0.0002])\n"
     ]
    }
   ],
   "source": [
    "w = net[0].weight.data\n",
    "print('w的估计误差：', true_w - w.reshape(true_w.shape))\n",
    "b = net[0].bias.data\n",
    "print('b的估计误差：', true_b - b)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T10:32:00.787738100Z",
     "start_time": "2023-09-20T10:32:00.771676200Z"
    }
   },
   "id": "f594c4e90d943a9b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 小结\n",
    "\n",
    "- 我们可以使用PyTorch的高级API更简洁地实现模型。\n",
    "- 在PyTorch中，data模块提供了数据处理工具，nn模块定义了大量的神经网络层和常见损失函数。\n",
    "- 我们可以通过_结尾的方法将参数替换，从而初始化参数。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a4f629424a38530"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2, loss: 0.00028365899925120175\n",
      "epoch: 3, loss: 0.00010428927635075524\n",
      "epoch: 4, loss: 0.00010341791494283825\n",
      "w的估计误差： tensor([0.0004, 0.0003])\n",
      "b的估计误差： tensor([0.0004])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(nn.Linear(2, 1))\n",
    "\n",
    "net[0].weight.data.normal_(0, 0.01)\n",
    "net[0].bias.data.fill_(0)\n",
    "\n",
    "# 累积误差\n",
    "loss = nn.MSELoss(reduction='sum')\n",
    "\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=0.03)\n",
    "\n",
    "num_epochs = 3\n",
    "for i in range(num_epochs):\n",
    "    for X, y in data_iter:\n",
    "        # 前向传播，求预测\n",
    "        y_hat = net(X)\n",
    "        # 计算损失\n",
    "        l = loss(y_hat, y)\n",
    "        # 梯度清零\n",
    "        trainer.zero_grad()\n",
    "        # 反向传播\n",
    "        l.backward()\n",
    "        # 更新参数\n",
    "        trainer.step()\n",
    "    train_y_hat = net(features)\n",
    "    train_l = loss(train_y_hat, labels)\n",
    "    print(f'epoch: {epoch + i}, loss: {train_l}')\n",
    "\n",
    "w = net[0].weight.data\n",
    "print('w的估计误差：', true_w - w.reshape(true_w.shape))\n",
    "b = net[0].bias.data\n",
    "print('b的估计误差：', true_b - b)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T11:03:49.078700500Z",
     "start_time": "2023-09-20T11:03:48.950298600Z"
    }
   },
   "id": "7cd1b920fe3cd9e9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 练习\n",
    "\n",
    "1. 如果将小批量的总损失替换为小批量损失的平均值，需要如何更改学习率？"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4df373bcf906869"
  },
  {
   "cell_type": "markdown",
   "source": [
    "将损失函数的定义方式中的reduction字段改为 'sum'的话，会使得梯度值放大为原来的num_example倍\n",
    "\n",
    "使得在不改变学习率的情况下 很容易出现在最优解附近震荡的情况 降低学习效果\n",
    "\n",
    "- 将损失函数的计算方式定义为整个损失的和\n",
    "```python\n",
    "loss = nn.MSELoss(reduction='sum')\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=0.03)  #指定需要优化的参数 还有学习率\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter:\n",
    "        l = loss(net(X) ,y) #这里的net返回输入x经过定义的网络所计算出的值\n",
    "        trainer.zero_grad() #清除上一次的梯度值 避免叠加上下面的数据所带来的梯度\n",
    "        l.backward() #损失函数进行反向传播 求参数的梯度 \n",
    "        trainer.step()  #trainer步进 根据指定的优化算法进行参数的寻优\n",
    "    l = loss(net(features), labels) #根据上面的参数优化结果计算参数对整个数据集的拟合状态 以loss进行反映\n",
    "    print(f'epoch {epoch + 1}, loss {l:f}')\n",
    "```\n",
    "\n",
    "- 在上述情况下改变学习率\n",
    "```python\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=0.03/10)  #指定需要优化的参数 还有学习率\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter:\n",
    "        l = loss(net(X), y)  #这里的net返回输入x经过定义的网络所计算出的值\n",
    "        trainer.zero_grad()  #清除上一次的梯度值 避免叠加上下面的数据所带来的梯度\n",
    "        l.backward()  #损失函数进行反向传播 求参数的梯度\n",
    "        trainer.step()  #trainer步进 根据指定的优化算法进行参数的寻优\n",
    "    l = loss(net(features), labels)  #根据上面的参数优化结果计算参数对整个数据集的拟合状态 以loss进行反映\n",
    "    print(f'epoch {epoch + 1}, loss {l:f}')\n",
    "```\n",
    "\n",
    "### 总结\n",
    "\n",
    "明显可见地选用sum而不是mean将很大程度影响模型的学习效果\n",
    "\n",
    "原因：若不将损失函数采用均值 将会使得参数的梯度过大\n",
    "\n",
    "梯度经过放大后 原有的学习率显得过大 使得其出现了振荡 即步长过长导致\n",
    "\n",
    "在最优解的附近震荡 而无法高效逼近最优点\n",
    "\n",
    "- 以平均值作为损失函数\n",
    "```text\n",
    "epoch 1, loss 0.000104\n",
    "epoch 2, loss 0.000104\n",
    "epoch 3, loss 0.000104\n",
    "```\n",
    "\n",
    "- 以和作为损失函数，不改变学习率\n",
    "```text\n",
    "epoch 1, loss 0.108244\n",
    "epoch 2, loss 0.123402\n",
    "epoch 3, loss 0.105799\n",
    "```\n",
    "\n",
    "- 以和作为损失函数，改变学习率\n",
    "```text\n",
    "epoch 1, loss 0.096369\n",
    "epoch 2, loss 0.096502\n",
    "epoch 3, loss 0.095959\n",
    "```\n",
    "\n",
    "总结：默认情况下以mean作为损失的计算结果更好"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "275749424850d0b7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. 查看深度学习框架文档，它们提供了哪些损失函数和初始化方法？用Huber损失代替原损失，即\n",
    "\n",
    "$$\n",
    "l(y,y')=\\begin{cases}\n",
    "|y-y'|-\\frac{\\sigma}{2},\\quad if|y-y'|\\gt\\sigma \\\\\n",
    "\\frac{1}{2\\sigma}(y-y')^2, \\quad 其他情况\n",
    "\\end{cases}\n",
    "\\tag{3.3.1}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6a1f5996b4249c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "损失函数：\n",
    "> torch.nn包\n",
    "\n",
    "|                                函数                                |                                              作用                                              |\n",
    "|:----------------------------------------------------------------:|:--------------------------------------------------------------------------------------------:|\n",
    "|                    L1Loss(size_average=True)                     |                                          差的绝对值的平均值                                           |\n",
    "|                    MSELoss(size_average=True)                    |                                             均方误差                                             |\n",
    "|         CrossEntropyLoss(weight=None, size_average=True)         |                                  将LogSoftMax和NLLLoss集成到一个类                                   |\n",
    "|             NLLLoss(weight=None, size_average=True)              |                                   负的log likelihood loss损失                                    |\n",
    "|            NLLLoss2d(weight=None, size_average=True)             |                      对于图片的 negative log likehood loss。计算每个像素的 NLL loss                       |\n",
    "|            KLDivLoss(weight=None, size_average=True)             |                                           KL 散度损失                                            |\n",
    "|             BCELoss(weight=None, size_average=True)              |                                            二进制交叉熵                                            |\n",
    "|          MarginRankingLoss(margin=0, size_average=True)          | 创建一个标准，给定输入 $x1$,$x2$两个1-D mini-batch Tensor's，和一个$y$(1-D mini-batch tensor) ,$y$里面的值只能是-1或1 |\n",
    "|              HingeEmbeddingLoss(size_average=True)               |         给定一个输入 $x$(2-D mini-batch tensor)和对应的 标签 $y$ (1-D tensor,1,-1)，此函数用来计算之间的损失值         |\n",
    "|             MultiLabelMarginLoss(size_average=True)              |                            计算多标签分类的 hinge loss(margin-based loss)                            |\n",
    "|                 SmoothL1Loss(size_average=True)                  |                                          平滑版L1 loss                                          |\n",
    "|                SoftMarginLoss(size_average=True)                 |                                 创建一个标准，用来优化2分类的logistic loss                                 |\n",
    "|     MultiLabelSoftMarginLoss(weight=None, size_average=True)     |                    创建一个标准，基于输入x和目标y的 max-entropy，优化多标签 one-versus-all 的损失                    |\n",
    "|         CosineEmbeddingLoss(margin=0, size_average=True)         |                                     使用cosine距离测量两个输入是否相似                                     |\n",
    "| MultiMarginLoss(p=1, margin=1, weight=None, size_average=True)   |                  计算multi-class classification的hinge loss（magin-based loss）                   |\n",
    "\n",
    "初始化方法：\n",
    "\n",
    "| nonlinearity |             gain             |\n",
    "|:------------:|:----------------------------:|\n",
    "|    linear    |              \t1              |\n",
    "| conv{1,2,3}d |              \t1              |\n",
    "|   sigmoid    |              \t1              |\n",
    "|     tanh     |             \t5/3             |\n",
    "|    relu\t   |           sqrt(2)            |\n",
    "|  leaky_relu  | sqrt(2/(1+negative_slope^2)) |"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7226c8be33739ac"
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. 如何访问线性回归的梯度？"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d6e6ff2274d0d2f"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[-0.0048,  0.0053]]), tensor([0.0050]))"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.grad, net[0].bias.grad"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T11:26:06.516070100Z",
     "start_time": "2023-09-20T11:26:06.466327700Z"
    }
   },
   "id": "b11047763e02c7f9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f438666d151a8ba0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
